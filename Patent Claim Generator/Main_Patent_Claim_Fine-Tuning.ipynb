{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Patent Claim Generation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "taV5X5MYJJNx",
        "outputId": "0e2d4b79-f329-465a-9f02-58940c229958",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# authorize access to the drive\n",
        "# This allows me to access files (Data) in my google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW9lOmu-DzHR",
        "outputId": "673a7410-4005-4cfc-833d-fe42e5971087",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# This downloads Hugging Face Transformers, tools we need to fine-tune GPT-2\n",
        "!pip3 install git+https://github.com/huggingface/transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-pbj2go5y\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-pbj2go5y\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (4.41.1)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 20.4MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 52.9MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 49.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (20.4)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (3.12.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.0) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.5.0) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.5.0) (50.3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (3.0.4)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-3.5.0-cp36-none-any.whl size=1311439 sha256=0cf8ecb5216634e66b412f9dbfccb8b34532a8097fc0030aacee7bec6f79763b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yy8sa5jq/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n",
            "Successfully built transformers\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=7f5d1df2143c0c04645c91644bd588098b226877371fee239627f150a084c42d\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqyVr-ugJHJZ",
        "outputId": "a4c3b622-6f75-464b-d25b-0c9742d3f4e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Allows me to go to the correct directory in my google drie\n",
        "%cd drive/My\\ Drive/School\\ Work/Fourth\\ Year/Capstone\\ Everything"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/School Work/Fourth Year/Capstone Everything\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iq2RCI-wEIPj",
        "outputId": "0a3ae6cb-3636-47c2-f9ea-f32a92d5f6fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# the fine-tuning code by Hugging Face that allows us to fine-tune\n",
        "# this file is already archieved after the most recent hugging face modelling se we need to replace the url\n",
        "# !wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/language-modeling/run_language_modeling.py\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/contrib/legacy/run_language_modeling.py"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-14 12:32:37--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/language-modeling/run_language_modeling.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2020-11-14 12:32:37 ERROR 404: Not Found.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QPzV5ugJL_V",
        "outputId": "10a3a821-e4f8-4818-d4a3-39afe72b4238",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "% ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Brian'\\''s Peer Feedback Template (10.28.21).gdoc'\n",
            "'Capstone Coversheet.gdoc'\n",
            "'Capstone Proposal (Junior Year).gdoc'\n",
            " \u001b[0m\u001b[01;34mclass\u001b[0m/\n",
            "'Dev Journal.gdoc'\n",
            " \u001b[01;34mModels\u001b[0m/\n",
            " \u001b[01;34mNone\u001b[0m/\n",
            " \u001b[01;34moutput_description\u001b[0m/\n",
            "\u001b[01;34m'Paper Drafts'\u001b[0m/\n",
            "'Punch Card.gsheet'\n",
            " run_language_modeling.py\n",
            " run_language_modeling.py.1\n",
            " run_language_modeling.py.2\n",
            " run_language_modeling.py.3\n",
            " \u001b[01;34mruns\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fWjHOEREKHF"
      },
      "source": [
        "# get dataset\n",
        "datapath = \"Models/Data\"\n",
        "train_path = datapath + \"/train.txt\"\n",
        "valid_path = datapath + \"/valid.txt\"\n",
        "output = \"./output\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZpI845vaDr6",
        "outputId": "33373e3a-e0d2-4b88-9416-3d50c343b7a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# %%bash \n",
        "# export DIRECTORY=Models/Data/Description_Data\n",
        "# ls $DIRECTORY"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cached_lm_GPT2Tokenizer_1024_description_train.txt\n",
            "cached_lm_GPT2Tokenizer_1024_description_train.txt.lock\n",
            "cached_lm_GPT2Tokenizer_1024_description_valid.txt\n",
            "cached_lm_GPT2Tokenizer_1024_description_valid.txt.lock\n",
            "description_test.txt\n",
            "description_train.txt\n",
            "description_valid.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no6UTkRDFLJU"
      },
      "source": [
        "# Main fine-tuning code using run_language_modeling.py\n",
        "%%bash\n",
        "export TRAIN_FILE=Models/Data/train.txt\n",
        "export TEST_FILE=Models/Data/valid.txt\n",
        "export MODEL_NAME=gpt2\n",
        "export OUTPUT_DIR=Models/Claims_model\n",
        "\n",
        "# python3 run_language_modeling.py \\\n",
        "#     --output_dir=$OUTPUT_DIR \\\n",
        "#     --model_type=$MODEL_NAME \\\n",
        "#     --model_name_or_path=$MODEL_NAME \\\n",
        "#     --do_train \\\n",
        "#     --train_data_file=$TRAIN_FILE \\\n",
        "#     --do_eval \\\n",
        "#     --eval_data_file=$TEST_FILE \\\n",
        "#     --per_gpu_train_batch_size=1 \\\n",
        "#     --save_steps=-1 \\\n",
        "#     --num_train_epochs=2\n",
        "\n",
        "python run_language_modeling.py \\\n",
        "    --output_dir=$OUTPUT_DIR \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=$MODEL_NAME \\\n",
        "    --do_train \\\n",
        "    --train_data_file=$TRAIN_FILE \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file=$TEST_FILE \\\n",
        "    --per_gpu_train_batch_size=1 \\\n",
        "    --save_steps=-1 \\\n",
        "    --num_train_epochs=2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yp6jDX_3HdVH",
        "outputId": "48790210-f8e0-4085-cc04-012c77d025b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# this allow us to generate sample patent claims \n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "OUTPUT_DIR = \"./output_description\"\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "# we use the GPT-2 model to load our fine-tuned model and get the tokenization\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(OUTPUT_DIR)\n",
        "model = GPT2LMHeadModel.from_pretrained(OUTPUT_DIR)\n",
        "model = model.to(device)\n",
        "                                        \n",
        "def generate(input_str, length=250, n=5):\n",
        "  '''\n",
        "  This is the main generation code using our model\n",
        "  '''\n",
        "  cur_ids = torch.tensor(tokenizer.encode(input_str)).unsqueeze(0).long().to(device)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for i in range(length):\n",
        "      outputs = model(cur_ids[:, -1024:], labels=cur_ids[:, -1024:])\n",
        "      loss, logits = outputs[:2]\n",
        "      softmax_logits = torch.softmax(logits[0,-1], dim=0)\n",
        "      next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n)\n",
        "      cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim=1)\n",
        "    output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
        "    output_text = tokenizer.decode(output_list)\n",
        "    return output_text\n",
        "\n",
        "def choose_from_top(probs, n=5):\n",
        "    '''\n",
        "    This chooses the top n choices and probabilistically select one choice\n",
        "    '''\n",
        "    ind = np.argpartition(probs, -n)[-n:]\n",
        "    top_prob = probs[ind]\n",
        "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
        "    choice = np.random.choice(n, 1, p = top_prob)\n",
        "    token_id = ind[choice][0]\n",
        "    return int(token_id)\n",
        "# print the generated text\n",
        "generated_text = generate(\"This aparatus \\n\", n=5)\n",
        "print(generated_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This aparatus \n",
            "<BOS> The present invention relates generally to organic electroluminescent devices, and more particularly to      an organic electroluminescent device comprising: a base      electrode; a cathode; and an an electron transport region between the      base electrode and the electron transport region. The electron transport      region includes a first region separated from the base electrode by a junction,      and a second region separated from the cathode by a junction. The      junction includes a first material in contact with the first region and a      second material in contact with the second region. The junction includes a      third material in contact with the third material in contact with the      junction. <EOS>\n",
            "<BOS> A method of manufacturing a light emitting device having a resin package      which provides an optical reflectivity equal to or more than 70% at a      wavelength between 350 nm and 800 nm after thermal curing, and in which a      resin part and a lead are formed in a substantially same plane\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN2jSDuYIzbe",
        "outputId": "05427fb0-9104-40ea-f0da-19f4b88540ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "% ls output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "config.json          merges.txt               tokenizer_config.json\n",
            "eval_results_lm.txt  pytorch_model.bin        training_args.bin\n",
            "log_history.json     special_tokens_map.json  vocab.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG6yJF4AIztA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}